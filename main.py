from dataclasses import dataclass
from typing import List, Tuple
from sentence_transformers import SentenceTransformer, util
import argparse
import pathlib
import torch
import sys

@dataclass
class Doc:
    id: int
    text: str

def load_lines(path: pathlib.Path) -> List[Doc]:
    """è®€å–æ–‡å­—æª”ï¼Œæ¯ä¸€è¡Œè¦–ç‚ºä¸€æ®µï¼›è‡ªå‹•åŽ»é™¤ç©ºè¡Œèˆ‡å‰å¾Œç©ºç™½ã€‚"""
    lines = []
    for i, raw in enumerate(path.read_text(encoding="utf-8").splitlines()):
        t = raw.strip()
        if t:
            lines.append(Doc(id=len(lines), text=t))
    if not lines:
        raise ValueError("è³‡æ–™ç‚ºç©ºï¼Œè«‹ç¢ºèªæª”æ¡ˆå…§å®¹ã€‚")
    return lines

def embed_texts(model: SentenceTransformer, docs: List[Doc]) -> torch.Tensor:
    texts = [d.text for d in docs]
    return model.encode(texts, convert_to_tensor=True, normalize_embeddings=True)

def search(
    model: SentenceTransformer,
    doc_embeddings: torch.Tensor,
    docs: List[Doc],
    query: str,
    top_k: int = 3
) -> List[Tuple[float, Doc]]:
    q = model.encode(query, convert_to_tensor=True, normalize_embeddings=True)
    scores = util.cos_sim(q, doc_embeddings)[0]   # shape: (N,)
    top_k = min(top_k, len(docs))
    top = torch.topk(scores, k=top_k)
    results = []
    for score, idx in zip(top.values.tolist(), top.indices.tolist()):
        results.append((float(score), docs[idx]))
    return results

def main():
    parser = argparse.ArgumentParser(description="Mini Semantic Searcher")
    parser.add_argument("--data", type=str, default="data/sample.txt", help="èªžæ–™æª”è·¯å¾‘ (æ¯è¡Œä¸€æ®µ)")
    parser.add_argument("--model", type=str, default="sentence-transformers/all-MiniLM-L6-v2",
                        help="SentenceTransformer æ¨¡åž‹åç¨±")
    parser.add_argument("--top_k", type=int, default=3, help="æ¯æ¬¡æŸ¥è©¢å›žå‚³å¹¾ç­†çµæžœ")
    args = parser.parse_args()

    data_path = pathlib.Path(args.data)
    if not data_path.exists():
        print(f"æ‰¾ä¸åˆ°èªžæ–™æª”ï¼š{data_path.resolve()}", file=sys.stderr)
        sys.exit(1)

    print("ðŸš€ è¼‰å…¥èªžæ–™...")
    docs = load_lines(data_path)

    print("ðŸ§  è¼‰å…¥æ¨¡åž‹ï¼ˆç¬¬ä¸€æ¬¡æœƒè‡ªå‹•ä¸‹è¼‰ï¼‰...")
    model = SentenceTransformer(args.model)

    print("ðŸ“¦ å‘é‡åŒ–èªžæ–™...")
    doc_embeddings = embed_texts(model, docs)

    print("\nâœ… æº–å‚™å®Œæˆï¼è¼¸å…¥æŸ¥è©¢é–‹å§‹æœå°‹ï¼Œè¼¸å…¥ q é›¢é–‹ã€‚")
    while True:
        try:
            query = input("\nðŸ”Ž æŸ¥è©¢ï¼š").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nå†è¦‹ï¼")
            break
        if not query:
            continue
        if query.lower() == "q":
            print("å†è¦‹ï¼")
            break

        results = search(model, doc_embeddings, docs, query, top_k=args.top_k)
        print("\nðŸ“š æœ€ç›¸é—œçµæžœï¼š")
        for rank, (score, doc) in enumerate(results, start=1):
            print(f"{rank:>2}. [ç›¸ä¼¼åº¦ {score:.4f}] {doc.text}")

if __name__ == "__main__":
    main()
